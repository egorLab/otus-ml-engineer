{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import language_modeling, WikiText2\n",
    "from torchtext.data import BPTTIterator, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda x: list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, \n",
    "             tokenize=tokenizer, \n",
    "             include_lengths=False, \n",
    "             use_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = WikiText2.splits(text_field=TEXT,\n",
    "                                    path='./wikitext/', train='train.txt',\n",
    "                                    validation='valid.txt', test='test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "batch_size = eval_batch_size = 128\n",
    "\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BPTTIterator.splits(\n",
    "    (train, val, test),\n",
    "    batch_size=batch_size,\n",
    "    bptt_len=sequence_length,\n",
    "    repeat=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'text', 'target'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_iter)); vars(b).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  9, 16],\n",
       "        [30, 21, 21],\n",
       "        [ 2,  2,  4],\n",
       "        [34, 20, 11],\n",
       "        [ 2,  5, 10]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.text[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 21, 21],\n",
       "        [ 2,  2,  4],\n",
       "        [34, 20, 11],\n",
       "        [ 2,  5, 10],\n",
       "        [72, 10,  2]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.target[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'text', 'target'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(val_iter)); vars(b).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.itos)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab.itos)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, data in enumerate(data_loader):\n",
    "        output, hidden = model(data.text.cuda())\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += criterion(output_flat, data.target.view(-1)).item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab.itos)\n",
    "    for batch, data in enumerate(train_iter):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data.text.cuda())\n",
    "        #print(output.shape)\n",
    "        #print(data.target.shape)\n",
    "        loss = criterion(output.view(-1, ntokens), data.target.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            print(total_loss)\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_iter) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long().cuda()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248.57534246575344"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "72584/292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " qr〈ñãγтยั₹†åS,tÉăCaá’ḥđ3♭@Rử:к戦.yოвEćCxβ†5礮fà’Ú`‘V \n",
      "\n",
      "359.86430406570435\n",
      "| epoch   1 |   100/   93 batches | lr 4.00 | loss  3.60 | ppl    36.55\n",
      "328.2866168022156\n",
      "| epoch   1 |   200/   93 batches | lr 4.00 | loss  3.28 | ppl    26.65\n",
      "325.0367991924286\n",
      "| epoch   1 |   300/   93 batches | lr 4.00 | loss  3.25 | ppl    25.80\n",
      "322.5218710899353\n",
      "| epoch   1 |   400/   93 batches | lr 4.00 | loss  3.23 | ppl    25.16\n",
      "322.15682220458984\n",
      "| epoch   1 |   500/   93 batches | lr 4.00 | loss  3.22 | ppl    25.07\n",
      "312.7357544898987\n",
      "| epoch   1 |   600/   93 batches | lr 4.00 | loss  3.13 | ppl    22.81\n",
      "300.65835642814636\n",
      "| epoch   1 |   700/   93 batches | lr 4.00 | loss  3.01 | ppl    20.22\n",
      "290.1759808063507\n",
      "| epoch   1 |   800/   93 batches | lr 4.00 | loss  2.90 | ppl    18.21\n",
      "282.0542154312134\n",
      "| epoch   1 |   900/   93 batches | lr 4.00 | loss  2.82 | ppl    16.79\n",
      "274.0940980911255\n",
      "| epoch   1 |  1000/   93 batches | lr 4.00 | loss  2.74 | ppl    15.50\n",
      "264.21088194847107\n",
      "| epoch   1 |  1100/   93 batches | lr 4.00 | loss  2.64 | ppl    14.04\n",
      "257.3124153614044\n",
      "| epoch   1 |  1200/   93 batches | lr 4.00 | loss  2.57 | ppl    13.11\n",
      "251.9567129611969\n",
      "| epoch   1 |  1300/   93 batches | lr 4.00 | loss  2.52 | ppl    12.42\n",
      "246.67348837852478\n",
      "| epoch   1 |  1400/   93 batches | lr 4.00 | loss  2.47 | ppl    11.78\n",
      "243.14263343811035\n",
      "| epoch   1 |  1500/   93 batches | lr 4.00 | loss  2.43 | ppl    11.38\n",
      "239.3864471912384\n",
      "| epoch   1 |  1600/   93 batches | lr 4.00 | loss  2.39 | ppl    10.96\n",
      "235.48937821388245\n",
      "| epoch   1 |  1700/   93 batches | lr 4.00 | loss  2.35 | ppl    10.54\n",
      "232.69613528251648\n",
      "| epoch   1 |  1800/   93 batches | lr 4.00 | loss  2.33 | ppl    10.25\n",
      "230.18995881080627\n",
      "| epoch   1 |  1900/   93 batches | lr 4.00 | loss  2.30 | ppl     9.99\n",
      "227.01465678215027\n",
      "| epoch   1 |  2000/   93 batches | lr 4.00 | loss  2.27 | ppl     9.68\n",
      "224.83975982666016\n",
      "| epoch   1 |  2100/   93 batches | lr 4.00 | loss  2.25 | ppl     9.47\n",
      "222.493084192276\n",
      "| epoch   1 |  2200/   93 batches | lr 4.00 | loss  2.22 | ppl     9.25\n",
      "221.57398676872253\n",
      "| epoch   1 |  2300/   93 batches | lr 4.00 | loss  2.22 | ppl     9.17\n",
      "218.8689591884613\n",
      "| epoch   1 |  2400/   93 batches | lr 4.00 | loss  2.19 | ppl     8.92\n",
      "217.58787751197815\n",
      "| epoch   1 |  2500/   93 batches | lr 4.00 | loss  2.18 | ppl     8.81\n",
      "215.55192518234253\n",
      "| epoch   1 |  2600/   93 batches | lr 4.00 | loss  2.16 | ppl     8.63\n",
      "213.84445023536682\n",
      "| epoch   1 |  2700/   93 batches | lr 4.00 | loss  2.14 | ppl     8.49\n",
      "211.07694220542908\n",
      "| epoch   1 |  2800/   93 batches | lr 4.00 | loss  2.11 | ppl     8.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.94 | valid ppl     6.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " iphilg feation ervened furnng sconceal . The pelic \n",
      "\n",
      "212.42774486541748\n",
      "| epoch   2 |   100/   93 batches | lr 4.00 | loss  2.12 | ppl     8.37\n",
      "208.14085924625397\n",
      "| epoch   2 |   200/   93 batches | lr 4.00 | loss  2.08 | ppl     8.02\n",
      "206.9755425453186\n",
      "| epoch   2 |   300/   93 batches | lr 4.00 | loss  2.07 | ppl     7.92\n",
      "206.03042936325073\n",
      "| epoch   2 |   400/   93 batches | lr 4.00 | loss  2.06 | ppl     7.85\n",
      "204.61805176734924\n",
      "| epoch   2 |   500/   93 batches | lr 4.00 | loss  2.05 | ppl     7.74\n",
      "203.39666306972504\n",
      "| epoch   2 |   600/   93 batches | lr 4.00 | loss  2.03 | ppl     7.64\n",
      "202.44381082057953\n",
      "| epoch   2 |   700/   93 batches | lr 4.00 | loss  2.02 | ppl     7.57\n",
      "201.47755002975464\n",
      "| epoch   2 |   800/   93 batches | lr 4.00 | loss  2.01 | ppl     7.50\n",
      "200.90495038032532\n",
      "| epoch   2 |   900/   93 batches | lr 4.00 | loss  2.01 | ppl     7.46\n",
      "199.9433994293213\n",
      "| epoch   2 |  1000/   93 batches | lr 4.00 | loss  2.00 | ppl     7.38\n",
      "198.24670457839966\n",
      "| epoch   2 |  1100/   93 batches | lr 4.00 | loss  1.98 | ppl     7.26\n",
      "197.77310276031494\n",
      "| epoch   2 |  1200/   93 batches | lr 4.00 | loss  1.98 | ppl     7.23\n",
      "196.74690353870392\n",
      "| epoch   2 |  1300/   93 batches | lr 4.00 | loss  1.97 | ppl     7.15\n",
      "195.04836070537567\n",
      "| epoch   2 |  1400/   93 batches | lr 4.00 | loss  1.95 | ppl     7.03\n",
      "195.16957819461823\n",
      "| epoch   2 |  1500/   93 batches | lr 4.00 | loss  1.95 | ppl     7.04\n",
      "194.70231938362122\n",
      "| epoch   2 |  1600/   93 batches | lr 4.00 | loss  1.95 | ppl     7.01\n",
      "193.5966421365738\n",
      "| epoch   2 |  1700/   93 batches | lr 4.00 | loss  1.94 | ppl     6.93\n",
      "193.25516784191132\n",
      "| epoch   2 |  1800/   93 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "193.22586143016815\n",
      "| epoch   2 |  1900/   93 batches | lr 4.00 | loss  1.93 | ppl     6.91\n",
      "191.5796353816986\n",
      "| epoch   2 |  2000/   93 batches | lr 4.00 | loss  1.92 | ppl     6.79\n",
      "191.68037748336792\n",
      "| epoch   2 |  2100/   93 batches | lr 4.00 | loss  1.92 | ppl     6.80\n",
      "190.94388890266418\n",
      "| epoch   2 |  2200/   93 batches | lr 4.00 | loss  1.91 | ppl     6.75\n",
      "191.1532884836197\n",
      "| epoch   2 |  2300/   93 batches | lr 4.00 | loss  1.91 | ppl     6.76\n",
      "189.31070220470428\n",
      "| epoch   2 |  2400/   93 batches | lr 4.00 | loss  1.89 | ppl     6.64\n",
      "189.11921393871307\n",
      "| epoch   2 |  2500/   93 batches | lr 4.00 | loss  1.89 | ppl     6.63\n",
      "189.1086926460266\n",
      "| epoch   2 |  2600/   93 batches | lr 4.00 | loss  1.89 | ppl     6.63\n",
      "188.45423579216003\n",
      "| epoch   2 |  2700/   93 batches | lr 4.00 | loss  1.88 | ppl     6.58\n",
      "186.9120752811432\n",
      "| epoch   2 |  2800/   93 batches | lr 4.00 | loss  1.87 | ppl     6.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.67 | valid ppl     5.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ing who foll shroughin the DMhists engord \" , his  \n",
      "\n",
      "188.8005782365799\n",
      "| epoch   3 |   100/   93 batches | lr 4.00 | loss  1.89 | ppl     6.61\n",
      "185.89585661888123\n",
      "| epoch   3 |   200/   93 batches | lr 4.00 | loss  1.86 | ppl     6.42\n",
      "185.8161619901657\n",
      "| epoch   3 |   300/   93 batches | lr 4.00 | loss  1.86 | ppl     6.41\n",
      "185.62922298908234\n",
      "| epoch   3 |   400/   93 batches | lr 4.00 | loss  1.86 | ppl     6.40\n",
      "185.00860571861267\n",
      "| epoch   3 |   500/   93 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
      "184.34880459308624\n",
      "| epoch   3 |   600/   93 batches | lr 4.00 | loss  1.84 | ppl     6.32\n",
      "184.51659893989563\n",
      "| epoch   3 |   700/   93 batches | lr 4.00 | loss  1.85 | ppl     6.33\n",
      "183.78499400615692\n",
      "| epoch   3 |   800/   93 batches | lr 4.00 | loss  1.84 | ppl     6.28\n",
      "183.9451560974121\n",
      "| epoch   3 |   900/   93 batches | lr 4.00 | loss  1.84 | ppl     6.29\n",
      "183.53358101844788\n",
      "| epoch   3 |  1000/   93 batches | lr 4.00 | loss  1.84 | ppl     6.27\n",
      "182.80221676826477\n",
      "| epoch   3 |  1100/   93 batches | lr 4.00 | loss  1.83 | ppl     6.22\n",
      "183.03472769260406\n",
      "| epoch   3 |  1200/   93 batches | lr 4.00 | loss  1.83 | ppl     6.24\n",
      "182.11905646324158\n",
      "| epoch   3 |  1300/   93 batches | lr 4.00 | loss  1.82 | ppl     6.18\n",
      "180.8369903564453\n",
      "| epoch   3 |  1400/   93 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "181.369078040123\n",
      "| epoch   3 |  1500/   93 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "181.3706338405609\n",
      "| epoch   3 |  1600/   93 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "180.71016693115234\n",
      "| epoch   3 |  1700/   93 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
      "180.79375326633453\n",
      "| epoch   3 |  1800/   93 batches | lr 4.00 | loss  1.81 | ppl     6.10\n",
      "181.35162365436554\n",
      "| epoch   3 |  1900/   93 batches | lr 4.00 | loss  1.81 | ppl     6.13\n",
      "180.1596393585205\n",
      "| epoch   3 |  2000/   93 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "180.6364643573761\n",
      "| epoch   3 |  2100/   93 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
      "180.18161058425903\n",
      "| epoch   3 |  2200/   93 batches | lr 4.00 | loss  1.80 | ppl     6.06\n",
      "180.59493601322174\n",
      "| epoch   3 |  2300/   93 batches | lr 4.00 | loss  1.81 | ppl     6.09\n",
      "178.89330744743347\n",
      "| epoch   3 |  2400/   93 batches | lr 4.00 | loss  1.79 | ppl     5.98\n",
      "178.99962043762207\n",
      "| epoch   3 |  2500/   93 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
      "179.80220746994019\n",
      "| epoch   3 |  2600/   93 batches | lr 4.00 | loss  1.80 | ppl     6.04\n",
      "179.03207314014435\n",
      "| epoch   3 |  2700/   93 batches | lr 4.00 | loss  1.79 | ppl     5.99\n",
      "177.9416173696518\n",
      "| epoch   3 |  2800/   93 batches | lr 4.00 | loss  1.78 | ppl     5.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.57 | valid ppl     4.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  about 1802 into the way general strerther . <eos> In  \n",
      "\n",
      "179.8796672821045\n",
      "| epoch   4 |   100/   93 batches | lr 4.00 | loss  1.80 | ppl     6.04\n",
      "177.39516258239746\n",
      "| epoch   4 |   200/   93 batches | lr 4.00 | loss  1.77 | ppl     5.89\n",
      "177.55783939361572\n",
      "| epoch   4 |   300/   93 batches | lr 4.00 | loss  1.78 | ppl     5.90\n",
      "177.5480626821518\n",
      "| epoch   4 |   400/   93 batches | lr 4.00 | loss  1.78 | ppl     5.90\n",
      "177.18369114398956\n",
      "| epoch   4 |   500/   93 batches | lr 4.00 | loss  1.77 | ppl     5.88\n",
      "176.54995107650757\n",
      "| epoch   4 |   600/   93 batches | lr 4.00 | loss  1.77 | ppl     5.84\n",
      "176.91158151626587\n",
      "| epoch   4 |   700/   93 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "176.44852447509766\n",
      "| epoch   4 |   800/   93 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
      "176.5337506532669\n",
      "| epoch   4 |   900/   93 batches | lr 4.00 | loss  1.77 | ppl     5.84\n",
      "176.4160599708557\n",
      "| epoch   4 |  1000/   93 batches | lr 4.00 | loss  1.76 | ppl     5.84\n",
      "175.84257423877716\n",
      "| epoch   4 |  1100/   93 batches | lr 4.00 | loss  1.76 | ppl     5.80\n",
      "176.36002266407013\n",
      "| epoch   4 |  1200/   93 batches | lr 4.00 | loss  1.76 | ppl     5.83\n",
      "175.65674149990082\n",
      "| epoch   4 |  1300/   93 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "174.4135786294937\n",
      "| epoch   4 |  1400/   93 batches | lr 4.00 | loss  1.74 | ppl     5.72\n",
      "175.10151994228363\n",
      "| epoch   4 |  1500/   93 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "175.22680974006653\n",
      "| epoch   4 |  1600/   93 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "174.55600571632385\n",
      "| epoch   4 |  1700/   93 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "174.7748122215271\n",
      "| epoch   4 |  1800/   93 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "175.662318110466\n",
      "| epoch   4 |  1900/   93 batches | lr 4.00 | loss  1.76 | ppl     5.79\n",
      "174.55304634571075\n",
      "| epoch   4 |  2000/   93 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "175.11630034446716\n",
      "| epoch   4 |  2100/   93 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "174.84129858016968\n",
      "| epoch   4 |  2200/   93 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "175.167307138443\n",
      "| epoch   4 |  2300/   93 batches | lr 4.00 | loss  1.75 | ppl     5.76\n",
      "173.67848074436188\n",
      "| epoch   4 |  2400/   93 batches | lr 4.00 | loss  1.74 | ppl     5.68\n",
      "173.87326872348785\n",
      "| epoch   4 |  2500/   93 batches | lr 4.00 | loss  1.74 | ppl     5.69\n",
      "174.67224955558777\n",
      "| epoch   4 |  2600/   93 batches | lr 4.00 | loss  1.75 | ppl     5.74\n",
      "174.51753330230713\n",
      "| epoch   4 |  2700/   93 batches | lr 4.00 | loss  1.75 | ppl     5.73\n",
      "173.0790581703186\n",
      "| epoch   4 |  2800/   93 batches | lr 4.00 | loss  1.73 | ppl     5.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.52 | valid ppl     4.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  route and some geres . <eos> An Down Conenting strual \n",
      "\n",
      "175.18418180942535\n",
      "| epoch   5 |   100/   93 batches | lr 4.00 | loss  1.75 | ppl     5.77\n",
      "172.82858753204346\n",
      "| epoch   5 |   200/   93 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "173.03980541229248\n",
      "| epoch   5 |   300/   93 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "173.05242097377777\n",
      "| epoch   5 |   400/   93 batches | lr 4.00 | loss  1.73 | ppl     5.64\n",
      "172.77952980995178\n",
      "| epoch   5 |   500/   93 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "172.3336329460144\n",
      "| epoch   5 |   600/   93 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "172.81012392044067\n",
      "| epoch   5 |   700/   93 batches | lr 4.00 | loss  1.73 | ppl     5.63\n",
      "172.49360692501068\n",
      "| epoch   5 |   800/   93 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "172.4280594587326\n",
      "| epoch   5 |   900/   93 batches | lr 4.00 | loss  1.72 | ppl     5.61\n",
      "172.31682288646698\n",
      "| epoch   5 |  1000/   93 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "171.89298057556152\n",
      "| epoch   5 |  1100/   93 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "172.59706270694733\n",
      "| epoch   5 |  1200/   93 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "171.861563205719\n",
      "| epoch   5 |  1300/   93 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "170.58262014389038\n",
      "| epoch   5 |  1400/   93 batches | lr 4.00 | loss  1.71 | ppl     5.51\n",
      "171.35139572620392\n",
      "| epoch   5 |  1500/   93 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "171.56434512138367\n",
      "| epoch   5 |  1600/   93 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "171.02388310432434\n",
      "| epoch   5 |  1700/   93 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "171.0342639684677\n",
      "| epoch   5 |  1800/   93 batches | lr 4.00 | loss  1.71 | ppl     5.53\n",
      "172.2814201116562\n",
      "| epoch   5 |  1900/   93 batches | lr 4.00 | loss  1.72 | ppl     5.60\n",
      "171.23578357696533\n",
      "| epoch   5 |  2000/   93 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "171.77714705467224\n",
      "| epoch   5 |  2100/   93 batches | lr 4.00 | loss  1.72 | ppl     5.57\n",
      "171.4103842973709\n",
      "| epoch   5 |  2200/   93 batches | lr 4.00 | loss  1.71 | ppl     5.55\n",
      "171.93373703956604\n",
      "| epoch   5 |  2300/   93 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "170.44500398635864\n",
      "| epoch   5 |  2400/   93 batches | lr 4.00 | loss  1.70 | ppl     5.50\n",
      "170.5095157623291\n",
      "| epoch   5 |  2500/   93 batches | lr 4.00 | loss  1.71 | ppl     5.50\n",
      "171.59160101413727\n",
      "| epoch   5 |  2600/   93 batches | lr 4.00 | loss  1.72 | ppl     5.56\n",
      "171.16672801971436\n",
      "| epoch   5 |  2700/   93 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "169.97592675685883\n",
      "| epoch   5 |  2800/   93 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.49 | valid ppl     4.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  chomical later strenk reconstruction , the top an \n",
      "\n",
      "172.00189089775085\n",
      "| epoch   6 |   100/   93 batches | lr 4.00 | loss  1.72 | ppl     5.58\n",
      "169.7729504108429\n",
      "| epoch   6 |   200/   93 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "170.01787841320038\n",
      "| epoch   6 |   300/   93 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "170.3545333147049\n",
      "| epoch   6 |   400/   93 batches | lr 4.00 | loss  1.70 | ppl     5.49\n",
      "169.83300065994263\n",
      "| epoch   6 |   500/   93 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "169.5531268119812\n",
      "| epoch   6 |   600/   93 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "170.03475213050842\n",
      "| epoch   6 |   700/   93 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "169.5758444070816\n",
      "| epoch   6 |   800/   93 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "169.7174402475357\n",
      "| epoch   6 |   900/   93 batches | lr 4.00 | loss  1.70 | ppl     5.46\n",
      "169.50551629066467\n",
      "| epoch   6 |  1000/   93 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "169.23777329921722\n",
      "| epoch   6 |  1100/   93 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "170.07463097572327\n",
      "| epoch   6 |  1200/   93 batches | lr 4.00 | loss  1.70 | ppl     5.48\n",
      "169.39895224571228\n",
      "| epoch   6 |  1300/   93 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "168.00494420528412\n",
      "| epoch   6 |  1400/   93 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "168.75923109054565\n",
      "| epoch   6 |  1500/   93 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "169.0019770860672\n",
      "| epoch   6 |  1600/   93 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "168.5594800710678\n",
      "| epoch   6 |  1700/   93 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "168.95825135707855\n",
      "| epoch   6 |  1800/   93 batches | lr 4.00 | loss  1.69 | ppl     5.42\n",
      "169.84005725383759\n",
      "| epoch   6 |  1900/   93 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "168.6560970544815\n",
      "| epoch   6 |  2000/   93 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "169.44030570983887\n",
      "| epoch   6 |  2100/   93 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "169.11693465709686\n",
      "| epoch   6 |  2200/   93 batches | lr 4.00 | loss  1.69 | ppl     5.43\n",
      "169.5482805967331\n",
      "| epoch   6 |  2300/   93 batches | lr 4.00 | loss  1.70 | ppl     5.45\n",
      "168.0495913028717\n",
      "| epoch   6 |  2400/   93 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "168.33088755607605\n",
      "| epoch   6 |  2500/   93 batches | lr 4.00 | loss  1.68 | ppl     5.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.3004608154297\n",
      "| epoch   6 |  2600/   93 batches | lr 4.00 | loss  1.69 | ppl     5.44\n",
      "168.80155491828918\n",
      "| epoch   6 |  2700/   93 batches | lr 4.00 | loss  1.69 | ppl     5.41\n",
      "167.75915372371674\n",
      "| epoch   6 |  2800/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.47 | valid ppl     4.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  elected in licked from European , and merized , t \n",
      "\n",
      "170.01217257976532\n",
      "| epoch   7 |   100/   93 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "167.61849164962769\n",
      "| epoch   7 |   200/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "168.1452248096466\n",
      "| epoch   7 |   300/   93 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "168.2704395055771\n",
      "| epoch   7 |   400/   93 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "167.9475440979004\n",
      "| epoch   7 |   500/   93 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "167.60239398479462\n",
      "| epoch   7 |   600/   93 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "167.81203293800354\n",
      "| epoch   7 |   700/   93 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "167.51381194591522\n",
      "| epoch   7 |   800/   93 batches | lr 4.00 | loss  1.68 | ppl     5.34\n",
      "167.6809093952179\n",
      "| epoch   7 |   900/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "167.61825704574585\n",
      "| epoch   7 |  1000/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "167.4090564250946\n",
      "| epoch   7 |  1100/   93 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "168.02755498886108\n",
      "| epoch   7 |  1200/   93 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "167.6183409690857\n",
      "| epoch   7 |  1300/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "166.1296478509903\n",
      "| epoch   7 |  1400/   93 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "167.04373383522034\n",
      "| epoch   7 |  1500/   93 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "167.1967717409134\n",
      "| epoch   7 |  1600/   93 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "166.82507240772247\n",
      "| epoch   7 |  1700/   93 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "167.00584316253662\n",
      "| epoch   7 |  1800/   93 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "168.10073864459991\n",
      "| epoch   7 |  1900/   93 batches | lr 4.00 | loss  1.68 | ppl     5.37\n",
      "166.9226871728897\n",
      "| epoch   7 |  2000/   93 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "167.77706217765808\n",
      "| epoch   7 |  2100/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "167.33035945892334\n",
      "| epoch   7 |  2200/   93 batches | lr 4.00 | loss  1.67 | ppl     5.33\n",
      "167.81509017944336\n",
      "| epoch   7 |  2300/   93 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "166.31196796894073\n",
      "| epoch   7 |  2400/   93 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "166.6062537431717\n",
      "| epoch   7 |  2500/   93 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "167.6970829963684\n",
      "| epoch   7 |  2600/   93 batches | lr 4.00 | loss  1.68 | ppl     5.35\n",
      "167.22403252124786\n",
      "| epoch   7 |  2700/   93 batches | lr 4.00 | loss  1.67 | ppl     5.32\n",
      "166.03438651561737\n",
      "| epoch   7 |  2800/   93 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.45 | valid ppl     4.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " n remains amount leasts that <unk> and the imsix ' \n",
      "\n",
      "168.3052957057953\n",
      "| epoch   8 |   100/   93 batches | lr 4.00 | loss  1.68 | ppl     5.38\n",
      "166.03670191764832\n",
      "| epoch   8 |   200/   93 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "166.46436369419098\n",
      "| epoch   8 |   300/   93 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "166.60198509693146\n",
      "| epoch   8 |   400/   93 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "166.34262692928314\n",
      "| epoch   8 |   500/   93 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "166.0574231147766\n",
      "| epoch   8 |   600/   93 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "166.43136036396027\n",
      "| epoch   8 |   700/   93 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "166.14814615249634\n",
      "| epoch   8 |   800/   93 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "166.18160951137543\n",
      "| epoch   8 |   900/   93 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "165.96127152442932\n",
      "| epoch   8 |  1000/   93 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "165.85208523273468\n",
      "| epoch   8 |  1100/   93 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "166.71143293380737\n",
      "| epoch   8 |  1200/   93 batches | lr 4.00 | loss  1.67 | ppl     5.30\n",
      "166.0462417602539\n",
      "| epoch   8 |  1300/   93 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "164.64077270030975\n",
      "| epoch   8 |  1400/   93 batches | lr 4.00 | loss  1.65 | ppl     5.19\n",
      "165.5173225402832\n",
      "| epoch   8 |  1500/   93 batches | lr 4.00 | loss  1.66 | ppl     5.23\n",
      "165.8092703819275\n",
      "| epoch   8 |  1600/   93 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "165.3445463180542\n",
      "| epoch   8 |  1700/   93 batches | lr 4.00 | loss  1.65 | ppl     5.22\n",
      "165.48050498962402\n",
      "| epoch   8 |  1800/   93 batches | lr 4.00 | loss  1.65 | ppl     5.23\n",
      "166.50019550323486\n",
      "| epoch   8 |  1900/   93 batches | lr 4.00 | loss  1.67 | ppl     5.29\n",
      "165.51949894428253\n",
      "| epoch   8 |  2000/   93 batches | lr 4.00 | loss  1.66 | ppl     5.23\n",
      "166.2426620721817\n",
      "| epoch   8 |  2100/   93 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "165.99907767772675\n",
      "| epoch   8 |  2200/   93 batches | lr 4.00 | loss  1.66 | ppl     5.26\n",
      "166.3498593568802\n",
      "| epoch   8 |  2300/   93 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "164.9869703054428\n",
      "| epoch   8 |  2400/   93 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "165.1465344429016\n",
      "| epoch   8 |  2500/   93 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "166.32009935379028\n",
      "| epoch   8 |  2600/   93 batches | lr 4.00 | loss  1.66 | ppl     5.28\n",
      "165.85226035118103\n",
      "| epoch   8 |  2700/   93 batches | lr 4.00 | loss  1.66 | ppl     5.25\n",
      "164.79459607601166\n",
      "| epoch   8 |  2800/   93 batches | lr 4.00 | loss  1.65 | ppl     5.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.44 | valid ppl     4.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , mexiginal stander break in a north of the @-@ m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train()\n",
    "    val_loss = evaluate(val_iter)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36_bayes)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
